{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6c3841",
   "metadata": {},
   "source": [
    "# Building a Complete ML Pipeline: Iris Classification\n",
    "\n",
    "## **What We're Building**\n",
    "A complete machine learning pipeline that:\n",
    "1. **Loads & explores** the famous Iris flower dataset\n",
    "2. **Preprocesses** data with standardization\n",
    "3. **Trains** a Random Forest classifier\n",
    "4. **Evaluates** model performance with multiple metrics\n",
    "5. **Visualizes** results with confusion matrix and feature importance\n",
    "\n",
    "## **The Iris Dataset**\n",
    "- **Goal**: Classify iris flowers into 3 species (setosa, versicolor, virginica)\n",
    "- **Features**: 4 measurements (sepal length/width, petal length/width)\n",
    "- **Samples**: 150 flowers (50 per species)\n",
    "- **Why This Dataset**: Perfect for learning ML fundamentals - clean, balanced, well-understood\n",
    "\n",
    "## **Pipeline Architecture**\n",
    "We'll build a **modular, reusable pipeline** using scikit-learn's best practices:\n",
    "- **Preprocessing Step**: StandardScaler for feature normalization\n",
    "- **Model Step**: RandomForestClassifier for robust classification\n",
    "- **Evaluation Step**: Comprehensive metrics and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03b96fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries for Our ML Pipeline\n",
    "import pandas as pd                    \n",
    "import numpy as np                    \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler   \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.pipeline import Pipeline          \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt   \n",
    "import seaborn as sns               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8de20d",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading & Exploration\n",
    "**Goal**: Load the Iris dataset and understand its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c56bad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_data() -> tuple[np.array, np.ndarray, list, list]:\n",
    "    \"\"\"\n",
    "    Load the Iris dataset and prepare it for ML pipeline.\n",
    "    \n",
    "    Returns:        \n",
    "        tuple: Features (X), target labels (y), feature names, target names\n",
    "    \"\"\"\n",
    "    iris = load_iris()\n",
    "    \n",
    "    # Create DataFrame for better data exploration\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['target'] = iris.target\n",
    "    df['species'] = df['target'].map({0: iris.target_names[0], \n",
    "                                     1: iris.target_names[1], \n",
    "                                     2: iris.target_names[2]})\n",
    "    return iris.data, iris.target, iris.feature_names, iris.target_names\n",
    "\n",
    "def create_train_test_split(X: np.array, y: np.ndarray, test_size: float = 0.2, random_state: int = 42) -> tuple:\n",
    "    \"\"\"\n",
    "    Split dataset into training and testing sets with stratification.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target labels  \n",
    "        test_size: Proportion for testing (default: 20%)\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # stratify=y ensures equal class distribution in train/test splits\n",
    "    # TODO: Use train_test_split to split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Load data and create train/test split\n",
    "X, y, feature_names, target_names = load_and_explore_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5ac785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary:\n",
      "  Total samples: 150\n",
      "  Features: 4\n",
      "  Classes: 3\n",
      "\n",
      " Target classes: [np.str_('setosa'), np.str_('versicolor'), np.str_('virginica')]\n",
      "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      " First 5 samples:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sepal length (cm)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sepal width (cm)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "petal length (cm)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "petal width (cm)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "species",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "760450a1-7d47-4b19-86f4-a4788c396f84",
       "rows": [
        [
         "0",
         "5.1",
         "3.5",
         "1.4",
         "0.2",
         "0",
         "setosa"
        ],
        [
         "1",
         "4.9",
         "3.0",
         "1.4",
         "0.2",
         "0",
         "setosa"
        ],
        [
         "2",
         "4.7",
         "3.2",
         "1.3",
         "0.2",
         "0",
         "setosa"
        ],
        [
         "3",
         "4.6",
         "3.1",
         "1.5",
         "0.2",
         "0",
         "setosa"
        ],
        [
         "4",
         "5.0",
         "3.6",
         "1.4",
         "0.2",
         "0",
         "setosa"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target species  \n",
       "0       0  setosa  \n",
       "1       0  setosa  \n",
       "2       0  setosa  \n",
       "3       0  setosa  \n",
       "4       0  setosa  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset Overview\n",
    "print(\"Dataset Summary:\")\n",
    "print(f\"  Total samples: {X.shape[0]}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Classes: {len(target_names)}\")\n",
    "\n",
    "print(f\"\\n Target classes: {list(target_names)}\")\n",
    "print(f\"Features: {list(feature_names)}\")\n",
    "\n",
    "# print(f\"\\n Training set: {X_train.shape[0]} samples\")\n",
    "#print(f\" Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "df['species'] = df['target'].map({0: target_names[0], \n",
    "                                 1: target_names[1], \n",
    "                                 2: target_names[2]})\n",
    "\n",
    "print(f\"\\n First 5 samples:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513637a",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing  \n",
    "**Goal**: Standardize features so all measurements are on the same scale\n",
    "- **Why**: Different features have different units (cm) and ranges\n",
    "- **Method**: StandardScaler transforms data to mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8cf885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_pipeline():\n",
    "    \"\"\"\n",
    "    Create preprocessing pipeline with feature standardization.\n",
    "    \n",
    "    StandardScaler ensures all features have:\n",
    "    - Mean = 0 (centered)\n",
    "    - Standard deviation = 1 (normalized)\n",
    "    \n",
    "    This prevents features with larger scales from dominating the model.\n",
    "    \"\"\"\n",
    "    # TODO: Initialize and return a StandardScaler\n",
    "    preprocessor = None\n",
    "    return preprocessor\n",
    "\n",
    "preprocessor = create_preprocessing_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227fb4e",
   "metadata": {},
   "source": [
    "## Step 3: Model Pipeline Creation\n",
    "**Goal**: Build a complete ML pipeline that chains preprocessing → model training\n",
    "- **Pipeline Benefits**: Ensures preprocessing steps are applied consistently\n",
    "- **RandomForest**: Ensemble method that combines many decision trees for robust predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ae12c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_pipeline(preprocessor):\n",
    "    \"\"\"\n",
    "    Create complete ML pipeline: preprocessing → model training.\n",
    "    \n",
    "    Pipeline Steps:\n",
    "    1. 'scaler': Standardize features using StandardScaler\n",
    "    2. 'rf': Train RandomForest classifier\n",
    "    \n",
    "    Args:\n",
    "        preprocessor: Preprocessing component (StandardScaler)\n",
    "        \n",
    "    Returns:\n",
    "        sklearn.Pipeline: Complete ML pipeline\n",
    "    \"\"\"\n",
    "    # TODO: Create a scikit-learn Pipeline with the following steps:\n",
    "    # 1. 'scaler': The preprocessor\n",
    "    # 2. 'rf': RandomForestClassifier with these parameters:\n",
    "    #     - n_estimators=100\n",
    "    #     - random_state=42\n",
    "    #     - max_depth=None\n",
    "    #     - min_samples_split=2\n",
    "    #     - min_samples_leaf=1\n",
    "    \n",
    "    pipeline = None\n",
    "    return pipeline\n",
    "\n",
    "def train_model(pipeline, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train the complete pipeline on training data.\n",
    "    \n",
    "    This automatically:\n",
    "    1. Fits StandardScaler on training data\n",
    "    2. Transforms training data  \n",
    "    3. Trains RandomForest on scaled data\n",
    "    \"\"\"\n",
    "    # TODO: Train the pipeline using X_train and y_train\n",
    "    \n",
    "    return pipeline \n",
    "\n",
    "pipeline = create_model_pipeline(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c0f21d",
   "metadata": {},
   "source": [
    "## Step 4: Model Evaluation\n",
    "**Goal**: Measure how well our model performs using multiple metrics\n",
    "- **Accuracy**: Overall correctness (correct predictions / total predictions)\n",
    "- **Precision**: Quality of positive predictions (avoids false alarms)\n",
    "- **Recall**: Completeness of positive predictions (catches all cases)\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balanced measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc1d2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(pipeline, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Trained ML pipeline\n",
    "        X_test: Test features\n",
    "        y_test: True test labels\n",
    "        \n",
    "    Returns:\n",
    "        tuple: confusion_matrix, accuracy, precision, recall, f1_score\n",
    "    \"\"\"\n",
    "    # Make predictions on test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    # TODO: Calculate the following metrics using y_test and y_pred\n",
    "    cm = None          # confusion_matrix\n",
    "    accuracy = None    # accuracy_score\n",
    "    precision = None   # precision_score (use average='weighted')\n",
    "    recall = None      # recall_score (use average='weighted')\n",
    "    f1 = None          # f1_score (use average='weighted')\n",
    "    \n",
    "    return cm, accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d36d06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(cm, target_names, feature_importance_df, fig_path='model_results.png'):\n",
    "    \"\"\"\n",
    "    Create professional visualizations for model interpretation.\n",
    "    \n",
    "    Visualizations:\n",
    "    1. Confusion Matrix: Shows prediction accuracy for each class\n",
    "    2. Feature Importance: Reveals which features the model relies on most\n",
    "    \n",
    "    Args:\n",
    "        cm: Confusion matrix\n",
    "        target_names: Class names\n",
    "        feature_importance_df: DataFrame with feature importance scores\n",
    "        fig_path: Path to save the visualization\n",
    "    \"\"\"    \n",
    "    # Create side-by-side plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Confusion Matrix Heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=target_names, yticklabels=target_names,\n",
    "                ax=axes[0])\n",
    "    axes[0].set_title('Confusion Matrix\\n(How often each class was predicted correctly)')\n",
    "    axes[0].set_xlabel('Predicted Class')\n",
    "    axes[0].set_ylabel('True Class')\n",
    "    \n",
    "    # 2. Feature Importance Bar Plot\n",
    "    sns.barplot(data=feature_importance_df, x='importance', y='feature', ax=axes[1])\n",
    "    axes[1].set_title('Feature Importance\\n(Which features matter most for predictions)')\n",
    "    axes[1].set_xlabel('Importance Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04084578",
   "metadata": {},
   "source": [
    "## Step 5: Complete Pipeline Execution\n",
    "**Goal**: Run the entire ML workflow from data loading to results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f8e16c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # TODO: Implement the complete ML workflow:\n",
    "    # 1. Load and explore data\n",
    "    # 2. Create train/test split\n",
    "    # 3. Create preprocessing pipeline\n",
    "    # 4. Create model pipeline\n",
    "    # 5. Train model\n",
    "    # 6. Evaluate model\n",
    "    # 7. Visualize results\n",
    "    \n",
    "    trained_pipeline = None\n",
    "    return trained_pipeline\n",
    "\n",
    "# Execute the complete workflow\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tried-ce-2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
